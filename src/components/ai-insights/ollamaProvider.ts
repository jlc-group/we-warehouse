/**
 * Ollama Provider - Local LLM Integration
 * 
 * Ollama ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô LLM ‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡∏ü‡∏£‡∏µ
 * 
 * ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Ollama:
 * 1. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å https://ollama.ai
 * 2. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô Ollama
 * 3. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î model: ollama pull llama3.2
 * 4. Ollama ‡∏à‡∏∞‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡πà http://localhost:11434
 */

// ========== TYPES ==========

export interface OllamaConfig {
  baseUrl: string;
  model: string;
  enabled: boolean;
}

export interface OllamaMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface OllamaResponse {
  model: string;
  created_at: string;
  message: {
    role: string;
    content: string;
  };
  done: boolean;
}

export interface OllamaModel {
  name: string;
  size: number;
  digest: string;
  modified_at: string;
}

// ========== DEFAULT CONFIG ==========

export const DEFAULT_OLLAMA_CONFIG: OllamaConfig = {
  baseUrl: 'http://localhost:11434',
  model: 'llama3.2',
  enabled: false
};

// ========== RECOMMENDED MODELS ==========

export const RECOMMENDED_MODELS = [
  { 
    name: 'llama3.2', 
    displayName: 'Llama 3.2 (3B)', 
    size: '2GB',
    description: '‡πÄ‡∏£‡πá‡∏ß ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ',
    recommended: true
  },
  { 
    name: 'llama3.2:1b', 
    displayName: 'Llama 3.2 (1B)', 
    size: '1.3GB',
    description: '‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å RAM ‡∏ô‡πâ‡∏≠‡∏¢',
    recommended: false
  },
  { 
    name: 'mistral', 
    displayName: 'Mistral 7B', 
    size: '4GB',
    description: '‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏â‡∏•‡∏≤‡∏î',
    recommended: false
  },
  { 
    name: 'qwen2.5:7b', 
    displayName: 'Qwen 2.5 (7B)', 
    size: '4.7GB',
    description: '‡∏î‡∏µ‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢',
    recommended: true
  },
  { 
    name: 'gemma2:2b', 
    displayName: 'Gemma 2 (2B)', 
    size: '1.6GB',
    description: '‡∏à‡∏≤‡∏Å Google ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏î‡∏µ',
    recommended: false
  }
];

// ========== OLLAMA PROVIDER CLASS ==========

export class OllamaProvider {
  private config: OllamaConfig;
  
  constructor(config: Partial<OllamaConfig> = {}) {
    this.config = { ...DEFAULT_OLLAMA_CONFIG, ...config };
  }
  
  /**
   * ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
   */
  async checkConnection(): Promise<{ connected: boolean; error?: string }> {
    try {
      const response = await fetch(`${this.config.baseUrl}/api/tags`, {
        method: 'GET',
        signal: AbortSignal.timeout(5000) // 5 second timeout
      });
      
      if (response.ok) {
        return { connected: true };
      }
      return { connected: false, error: `HTTP ${response.status}` };
    } catch (error: any) {
      if (error.name === 'AbortError') {
        return { connected: false, error: 'Connection timeout' };
      }
      return { connected: false, error: error.message || 'Connection failed' };
    }
  }
  
  /**
   * ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ models ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß
   */
  async getInstalledModels(): Promise<OllamaModel[]> {
    try {
      const response = await fetch(`${this.config.baseUrl}/api/tags`);
      if (!response.ok) throw new Error('Failed to fetch models');
      
      const data = await response.json();
      return data.models || [];
    } catch (error) {
      console.error('Error fetching Ollama models:', error);
      return [];
    }
  }
  
  /**
   * ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á Ollama
   */
  async chat(
    messages: OllamaMessage[],
    onStream?: (chunk: string) => void
  ): Promise<string> {
    const response = await fetch(`${this.config.baseUrl}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: this.config.model,
        messages,
        stream: !!onStream
      })
    });
    
    if (!response.ok) {
      throw new Error(`Ollama error: ${response.status} ${response.statusText}`);
    }
    
    // Streaming response
    if (onStream && response.body) {
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let fullResponse = '';
      
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim());
        
        for (const line of lines) {
          try {
            const json: OllamaResponse = JSON.parse(line);
            if (json.message?.content) {
              fullResponse += json.message.content;
              onStream(json.message.content);
            }
          } catch {
            // Skip invalid JSON lines
          }
        }
      }
      
      return fullResponse;
    }
    
    // Non-streaming response
    const data: OllamaResponse = await response.json();
    return data.message?.content || '';
  }
  
  /**
   * ‡∏™‡∏£‡πâ‡∏≤‡∏á System Prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Warehouse AI
   */
  buildSystemPrompt(context: any): string {
    return `‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Assistant ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏±‡∏á‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ WE-Warehouse

**‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:**
- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏ô‡∏Ñ‡∏•‡∏±‡∏á
- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢
- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏±‡∏á
- ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤

**‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:**
- ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
- ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö
- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå
- ‡πÉ‡∏ä‡πâ emoji ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ï‡∏£‡∏á‡πÜ
- ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÑ‡∏î‡πâ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°

${context ? `**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô:**
${JSON.stringify(context, null, 2)}` : ''}`;
  }
  
  /**
   * ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
   */
  async processQuestion(
    question: string,
    context: any,
    chatHistory: OllamaMessage[] = [],
    onStream?: (chunk: string) => void
  ): Promise<{ answer: string; usedOllama: boolean }> {
    // Check connection first
    const { connected, error } = await this.checkConnection();
    if (!connected) {
      return {
        answer: `‚ùå **‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama ‡πÑ‡∏î‡πâ**\n\n` +
          `üîß ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:\n` +
          `1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà\n` +
          `2. ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: \`ollama serve\`\n` +
          `3. ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÅ‡∏≠‡∏õ Ollama\n\n` +
          `üìç URL: ${this.config.baseUrl}\n` +
          `‚ùå Error: ${error}`,
        usedOllama: false
      };
    }
    
    try {
      const systemPrompt = this.buildSystemPrompt(context);
      
      const messages: OllamaMessage[] = [
        { role: 'system', content: systemPrompt },
        ...chatHistory.slice(-10), // Last 10 messages for context
        { role: 'user', content: question }
      ];
      
      const answer = await this.chat(messages, onStream);
      
      return {
        answer: answer || '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ',
        usedOllama: true
      };
    } catch (error: any) {
      console.error('Ollama processing error:', error);
      return {
        answer: `‚ùå **‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î**\n\n${error.message}`,
        usedOllama: false
      };
    }
  }
  
  // Getters/Setters
  getConfig(): OllamaConfig {
    return { ...this.config };
  }
  
  setConfig(config: Partial<OllamaConfig>): void {
    this.config = { ...this.config, ...config };
  }
  
  setModel(model: string): void {
    this.config.model = model;
  }
  
  setBaseUrl(url: string): void {
    this.config.baseUrl = url;
  }
  
  setEnabled(enabled: boolean): void {
    this.config.enabled = enabled;
  }
}

// ========== SINGLETON INSTANCE ==========

let ollamaInstance: OllamaProvider | null = null;

export function getOllamaProvider(config?: Partial<OllamaConfig>): OllamaProvider {
  if (!ollamaInstance) {
    ollamaInstance = new OllamaProvider(config);
  } else if (config) {
    ollamaInstance.setConfig(config);
  }
  return ollamaInstance;
}

// ========== HELPER FUNCTIONS ==========

/**
 * ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î model ‡∏ú‡πà‡∏≤‡∏ô Ollama API
 */
export async function pullModel(
  modelName: string, 
  baseUrl: string = DEFAULT_OLLAMA_CONFIG.baseUrl,
  onProgress?: (status: string, completed?: number, total?: number) => void
): Promise<boolean> {
  try {
    const response = await fetch(`${baseUrl}/api/pull`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ name: modelName, stream: true })
    });
    
    if (!response.ok || !response.body) {
      throw new Error('Failed to pull model');
    }
    
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      const chunk = decoder.decode(value);
      const lines = chunk.split('\n').filter(line => line.trim());
      
      for (const line of lines) {
        try {
          const json = JSON.parse(line);
          if (onProgress) {
            onProgress(
              json.status || 'Downloading...',
              json.completed,
              json.total
            );
          }
        } catch {
          // Skip invalid JSON
        }
      }
    }
    
    return true;
  } catch (error) {
    console.error('Error pulling model:', error);
    return false;
  }
}

/**
 * ‡∏•‡∏ö model
 */
export async function deleteModel(
  modelName: string,
  baseUrl: string = DEFAULT_OLLAMA_CONFIG.baseUrl
): Promise<boolean> {
  try {
    const response = await fetch(`${baseUrl}/api/delete`, {
      method: 'DELETE',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ name: modelName })
    });
    return response.ok;
  } catch (error) {
    console.error('Error deleting model:', error);
    return false;
  }
}



